{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns=None\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from dateutil.relativedelta import *\n",
    "from dateutil.rrule import *\n",
    "from datetime import datetime as dt\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "#import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../Proyecto-Taxis-NYC/data/yellow_t_2018-01.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_missings(df,plot=True,rotation=45,figsize=(10,5), * kwargs):\\n    labels,values,percent = list(),list(),list()\\n    if df.isna().sum().sum()>0:\\n        for column in df.columns:\\n            if df[column].isna().sum():\\n                labels.append(column)\\n                values.append(df[column].isna().sum())\\n                percent.append((df[column].isna().sum() / df.shape[0]) * 100)\\n        #Make a dataframe \\n        missings=pd.DataFrame({\\'Colúmnas\\':labels,\\'Valores faltantes\\':values,\\'PorcentajePerdido\\':percent}).sort_values(by=\\'PorcentajePerdido\\',ascending=False)\\n        \\n\\n        if plot:\\n            plt.figure(figsize=figsize)\\n            plot = sns.barplot(x=missings.Colúmnas,y=round(missings.PorcentajePerdido, 2), palette = \"Blues_r\")# .set_title(\\'Porcentaje de valores perdidos\\',size=22)\\n            for i in plot.patches:\\n                plot.annotate(i.get_height(),\\n                (i.get_x() + i.get_width() / 2, i.get_height()),\\n                ha = \\'center\\', va = \\'baseline\\', fontsize = 12,\\n                color = \\'black\\', xytext = (0,1),\\n                textcoords = \\'offset pixels\\')\\n            locs, labels = plt.xticks()\\n            plt.setp(labels, rotation=rotation,size=16)\\n            plt.xlabel(\\'Colúmnas\\',size=18)\\n            plt.ylabel(\\'Porcentaje\\',size=18)\\n            plot.set_title(\\'Porcentaje de valores perdidos\\',size=18)\\n\\n        return missings\\n    else:\\n        return False\\n        '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#esto se uso inicilamente para poder darnos una idea de los datos a tratar\n",
    "\n",
    "\"\"\"\n",
    "def get_missings(df,plot=True,rotation=45,figsize=(10,5), * kwargs):\n",
    "    labels,values,percent = list(),list(),list()\n",
    "    if df.isna().sum().sum()>0:\n",
    "        for column in df.columns:\n",
    "            if df[column].isna().sum():\n",
    "                labels.append(column)\n",
    "                values.append(df[column].isna().sum())\n",
    "                percent.append((df[column].isna().sum() / df.shape[0]) * 100)\n",
    "        #Make a dataframe \n",
    "        missings=pd.DataFrame({'Colúmnas':labels,'Valores faltantes':values,'PorcentajePerdido':percent}).sort_values(by='PorcentajePerdido',ascending=False)\n",
    "        \n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=figsize)\n",
    "            plot = sns.barplot(x=missings.Colúmnas,y=round(missings.PorcentajePerdido, 2), palette = \"Blues_r\")# .set_title('Porcentaje de valores perdidos',size=22)\n",
    "            for i in plot.patches:\n",
    "                plot.annotate(i.get_height(),\n",
    "                (i.get_x() + i.get_width() / 2, i.get_height()),\n",
    "                ha = 'center', va = 'baseline', fontsize = 12,\n",
    "                color = 'black', xytext = (0,1),\n",
    "                textcoords = 'offset pixels')\n",
    "            locs, labels = plt.xticks()\n",
    "            plt.setp(labels, rotation=rotation,size=16)\n",
    "            plt.xlabel('Colúmnas',size=18)\n",
    "            plt.ylabel('Porcentaje',size=18)\n",
    "            plot.set_title('Porcentaje de valores perdidos',size=18)\n",
    "\n",
    "        return missings\n",
    "    else:\n",
    "        return False\n",
    "        \"\"\"\n",
    "\n",
    "#get_missings(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la transformacion de los tipos de datos en el dataset de taxis para una mejor categorizacion y eliminamos las columnas irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_datime(df):\n",
    "    df[\"dayofmonth\"] = df['tpep_pickup_datetime'].apply(pd.to_datetime).dt.day\n",
    "    df[\"hora\"]= df.tpep_pickup_datetime.dt.floor(\"H\")\n",
    "\n",
    "    df.drop(columns=[\"congestion_surcharge\",\"airport_fee\",\"VendorID\",\"store_and_fwd_flag\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def t_int(df):\n",
    "    df['passenger_count'] = pd.to_numeric(df['passenger_count'], downcast=\"integer\" )\n",
    "    df['RatecodeID'] = pd.to_numeric(df['RatecodeID'], downcast=\"integer\" )\n",
    "    df['PULocationID'] = pd.to_numeric(df['PULocationID'], downcast=\"integer\" )\n",
    "    df['DOLocationID'] = pd.to_numeric(df['DOLocationID'], downcast=\"integer\" )\n",
    "    df['payment_type'] = pd.to_numeric(df['payment_type'], downcast=\"integer\" )\n",
    "    df['RatecodeID'] = pd.to_numeric(df['RatecodeID'], downcast=\"integer\" )\n",
    "    df['RatecodeID'] = pd.to_numeric(df['RatecodeID'], downcast=\"integer\" )\n",
    "    return df\n",
    "\n",
    "def t_float(df):\n",
    "    df['trip_distance'] = pd.to_numeric(df['trip_distance'], downcast=\"float\" )\n",
    "    df['fare_amount'] = pd.to_numeric(df['fare_amount'], downcast=\"float\" )\n",
    "    df['extra'] = pd.to_numeric(df['extra'], downcast=\"float\" )\n",
    "    df['mta_tax'] = pd.to_numeric(df['mta_tax'], downcast=\"float\" )\n",
    "    df['tip_amount'] = pd.to_numeric(df['tip_amount'], downcast=\"float\" )\n",
    "    df['tolls_amount'] = pd.to_numeric(df['tolls_amount'], downcast=\"float\" )\n",
    "    df['improvement_surcharge'] = pd.to_numeric(df['improvement_surcharge'], downcast=\"float\" )\n",
    "    df['total_amount'] = pd.to_numeric(round(df['total_amount'] ))\n",
    "    return df\n",
    "\n",
    "def total_transform(df):\n",
    "    df = t_datime(df)\n",
    "    df = t_int(df)\n",
    "    df = t_float(df)\n",
    "    return df\n",
    "\n",
    "df =  total_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una mascara del dataframe cada 3 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar = df[df.dayofmonth % 3 == 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorteamos los value por date y filtramos los que estan fuera del mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted(df):\n",
    "    global bol\n",
    "    bol = 1\n",
    "    df.sort_values(by='tpep_pickup_datetime', ascending=True, inplace=True)\n",
    "    df = df[df[\"tpep_pickup_datetime\"] >= '2018-01-01 00:00:00']\n",
    "    if bol == 1:\n",
    "        df = df[df[\"tpep_pickup_datetime\"] <= '2018-02-01 00:00:00']\n",
    "        bol = 0\n",
    "    return df\n",
    "\n",
    "df_Impar = sorted(df_Impar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De ahora en mas df_Impar es el df a usar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener los Boroughs de salida y de llegada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boroughs (df_Impar):\n",
    "    #se carga el csv con las zonas\n",
    "    zona = pd.read_csv(\"taxi+_zone_lookup.csv\")\n",
    "    #se borran las columnas que no usamos\n",
    "    zona.drop(columns=[\"Zone\",\"service_zone\"], inplace=True)\n",
    "    #cambiamos el nombre para poder hacer el merge\n",
    "    zona.rename(columns={\"LocationID\": \"PULocationID\"},inplace=True)\n",
    "    #se hace el merge y se crea la columnna resultante del merge \"Borough\"\n",
    "    df_Impar = pd.merge(df_Impar,zona, how=\"left\", on=[\"PULocationID\"])\n",
    "    #se le cambia el nombre a \"boroug\" para que haga referencia al borough de partida\n",
    "    df_Impar.rename(columns={\"Borough\": \"PUBorough\"},inplace=True)\n",
    "    #mismo procedimiento para borough de llegada\n",
    "    zona.rename(columns={\"PULocationID\": \"DOLocationID\"},inplace=True)\n",
    "    df_Impar = pd.merge(df_Impar,zona, how=\"left\", on=[\"DOLocationID\"])\n",
    "    df_Impar.rename(columns={\"Borough\": \"DOLocation\"},inplace=True)\n",
    "    return df_Impar\n",
    "\n",
    "df_Impar = boroughs(df_Impar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajamos con el clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentacion mismo nombre del clima en cada update\n",
    "\n",
    "clima = pd.read_csv(\"C:/Users/andre/OneDrive/Escritorio/proyecto final 3/Proyecto-Taxis-NYC/new york 2018-01-01 to 2018-02-01.csv\")\n",
    "clima.drop(columns=[\"name\",\"dew\",\"humidity\",\"precipprob\",\"preciptype\",\"severerisk\",\"uvindex\",\"icon\",\"stations\"], inplace=True)\n",
    "clima.drop(columns=[\"windgust\",\"windspeed\",\"winddir\",\"sealevelpressure\",\"cloudcover\",\"visibility\",\"solarradiation\",\"solarenergy\"], inplace=True)\n",
    "clima[\"datetime\"]=[x.replace('T',\" \") for x in clima[\"datetime\"]]\n",
    "clima.datetime = clima['datetime'].apply(pd.to_datetime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos los datos de clima para optimizar rendimiento y chequear que esten bien los formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_clima():\n",
    "    clima['temp'] = pd.to_numeric(clima['temp'], downcast=\"float\" )\n",
    "    clima['feelslike'] = pd.to_numeric(round(clima['feelslike'] ))\n",
    "    clima['precip'] = pd.to_numeric(clima['precip'], downcast=\"float\" )\n",
    "    clima['snow'] = pd.to_numeric(clima['snow'], downcast=\"float\" )\n",
    "    clima['snowdepth'] = pd.to_numeric(clima['snowdepth'], downcast=\"float\" )\n",
    "    clima[\"conditions\"]=clima[\"conditions\"].astype(str)\n",
    "    return clima\n",
    "\n",
    "clima = t_clima()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos restriccion por meses dentro del clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_clima(dat,clima):\n",
    "    #aplicar sort fecha\n",
    "    clima.sort_values(by='datetime', ascending=True, inplace=True)\n",
    "    date = dat\n",
    "    if (clima[\"datetime\"] >= date).any():\n",
    "        clima = clima[clima[\"datetime\"] >= date]\n",
    "        date_next= date + relativedelta(months=+1)\n",
    "        clima = clima[clima[\"datetime\"] <= date_next]\n",
    "        return clima\n",
    "    else:\n",
    "        date+= relativedelta(months=+1)\n",
    "        return control_clima(dat)\n",
    "\n",
    "clima = control_clima(dt(2018,1,1),clima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos columna auxiliar de hora dentro de la tabla clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clima in global\n",
    "\n",
    "def aux ():\n",
    "    \n",
    "    clima.rename(columns={\"datetime\": \"hora\"},inplace=True)\n",
    "    return clima\n",
    "\n",
    "clima = aux()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mergearon los datos del clima al df_Impar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar = pd.merge(df_Impar,clima, how=\"left\", on=[\"hora\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPUTACION DE VALORES Y ARREGLO DE OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se sacan los outliers para calcular los valores medios del avg_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear df sin outliers.\n",
    "Q1 = df_Impar[\"trip_distance\"].quantile(0.01)\n",
    "Q3 = df_Impar[\"trip_distance\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliersSup = (Q3 + (1.5*IQR)) \n",
    "mask = df_Impar[\"trip_distance\"]<outliersSup\n",
    "df_ImparsinOut = df_Impar[mask]\n",
    "\n",
    "#se sacaron outliers de trip distance\n",
    "\n",
    "Q1 = df_ImparsinOut[\"fare_amount\"].quantile(0.01)\n",
    "Q3 = df_ImparsinOut[\"fare_amount\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliersSup = (Q3 + (1.5*IQR)) \n",
    "mask = df_ImparsinOut[\"fare_amount\"]<outliersSup\n",
    "df_ImparsinOut = df_ImparsinOut[mask]\n",
    "\n",
    "#se sacaron outliers de fare amount\n",
    "Q1 = df_ImparsinOut[\"tip_amount\"].quantile(0.01)\n",
    "Q3 = df_ImparsinOut[\"tip_amount\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliersSup = (Q3 + (1.5*IQR)) \n",
    "mask = df_ImparsinOut[\"tip_amount\"]<outliersSup\n",
    "df_ImparsinOut = df_ImparsinOut[mask]\n",
    "\n",
    "#se sacaron outliers de tip amount\n",
    "Q1 = df_ImparsinOut[\"total_amount\"].quantile(0.01)\n",
    "Q3 = df_ImparsinOut[\"total_amount\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliersSup = (Q3 + (1.5*IQR)) \n",
    "mask = df_ImparsinOut[\"total_amount\"]<outliersSup\n",
    "df_ImparsinOut = df_ImparsinOut[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def millas(df):\n",
    "  df['km_dollar'] = df['trip_distance']/df['fare_amount']\n",
    "  return df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def arreglo (df_ImparsinOut, ID):\n",
    "    dfsinceros=df_ImparsinOut[df_ImparsinOut.fare_amount != 0]\n",
    "    dfsinceros=dfsinceros[dfsinceros.fare_amount>0]\n",
    "    dfsinceros = dfsinceros[dfsinceros.trip_distance != 0]\n",
    "\n",
    "    df = dfsinceros[dfsinceros.RatecodeID == ID]\n",
    "    df = millas(df)\n",
    "    return df\n",
    "\n",
    "df1 = arreglo(df_ImparsinOut, 1)\n",
    "df2 = arreglo(df_ImparsinOut, 2)\n",
    "df3 = arreglo(df_ImparsinOut, 3)\n",
    "df4 = arreglo(df_ImparsinOut, 4)\n",
    "df5 = arreglo(df_ImparsinOut, 5)\n",
    "\n",
    "avg_km1 =df1['km_dollar'].mean()\n",
    "avg_km2 =df2['km_dollar'].mean()\n",
    "avg_km3 =df3['km_dollar'].mean()\n",
    "avg_km4 =df4['km_dollar'].mean()\n",
    "avg_km5 =df5['km_dollar'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se borran los que tienen cero en trip distance y fare amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Impar[\"borrar\"] = df_Impar.apply(lambda row: 1 if row[\"fare_amount\"]==0  and row[\"trip_distance\"]==0 else 0, axis=1)\n",
    "df_Impar = df_Impar.drop(df_Impar[df_Impar.borrar == 1].index)\n",
    "df_Impar = df_Impar.drop(columns=\"borrar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kms(df_Impar, ID, avg_km):\n",
    "    df_Impar[\"trip_distance\"] = df_Impar.apply(lambda row: row[\"fare_amount\"]*avg_km if row[\"RatecodeID\"]==ID  and row[\"trip_distance\"]==0 else row[\"trip_distance\"], axis=1)\n",
    "    return df_Impar[\"trip_distance\"]\n",
    "\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 1, avg_km1)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 2, avg_km2)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 3, avg_km3)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 4, avg_km4)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 5, avg_km5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se borran las ID innecesarias y los valores negativos, dado que tienen algunas incongruencias en algunas fechas por lo cual no podemos afirmar que son cancelaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_values(df_Impar):   \n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.fare_amount<0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.trip_distance<0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.total_amount<0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.RatecodeID==6].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.RatecodeID==99].index)\n",
    "    return df_Impar\n",
    "\n",
    "df_Impar=neg_values(df_Impar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos los outliers dentro de las columnas que pueden presentar tales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lo que hay que hacer para imputar valores y tratar outliers es lo siguiente:\n",
    "sacamos los valores outliers y creamos un df aux. a ese df le borramos los valores con cero de la columna trip distance,\n",
    "luego calculamos es costo promedio de la milla por cada ratecodeid, lo imputamos en el df completo ( no importa si fareamount es un outlier, porque la imputacion de distancia tambien lo va a ser. por ende es posible que ese viaje termine descartado)\n",
    "luego se trabaja con las etapas de los outliers, ya sabiendo que en trip distance no hay ceros, pero si outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 1: marcar los outliers., y las tips no las podemos imputar asique las vamos a imputar con su valor medio en caso de ser outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratamiento de Outliers columna trip_distance\n",
    "Q1 = df_Impar[\"trip_distance\"].quantile(0.01)\n",
    "Q3 = df_Impar[\"trip_distance\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliers_Sup_trip = (Q3 + (1.5*IQR)) \n",
    "#se crea la columna marcador\n",
    "df_Impar[\"trip_distance_out\"] = df_Impar.apply(lambda row: 1 if row[\"trip_distance\"]>outliers_Sup_trip else 0, axis=1)\n",
    "\n",
    "#se marco trip distance, ahora vamos por fare amount\n",
    "\n",
    "#Tratamiento de Outliers columna fare_amount\n",
    "Q1 = df_Impar[\"fare_amount\"].quantile(0.01)\n",
    "Q3 = df_Impar[\"fare_amount\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliers_Sup_fare = (Q3 + (1.5*IQR)) \n",
    "#se marco fare amount\n",
    "df_Impar[\"fare_amount_out\"] = df_Impar.apply(lambda row: 1 if row[\"fare_amount\"]>outliers_Sup_fare else 0, axis=1)\n",
    "\n",
    "#Tratamiento de Outliers columna tip_amount\n",
    "Q1 = df_Impar[\"tip_amount\"].quantile(0.01)\n",
    "Q3 = df_Impar[\"tip_amount\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliers_Sup_tip = (Q3 + (1.5*IQR)) \n",
    "#se reemplaza las tips outliers por el valor medio de las tips sin tener encuenta los ceros, la propina media es de 3 dolares.\n",
    "mask = df_Impar[\"tip_amount\"]<outliers_Sup_tip\n",
    "ventaCantSinOut = df_Impar[mask]\n",
    "ventaCantSinOut = ventaCantSinOut[ventaCantSinOut.tip_amount != 0]\n",
    "cantidadMedia = round(ventaCantSinOut.tip_amount.mean())\n",
    "df_Impar.loc[(df_Impar['tip_amount'] >= outliers_Sup_tip),\"tip_amount\"] = cantidadMedia #3\n",
    "\n",
    "#se marco tip amount, ahora le toca a total amount\n",
    "\n",
    "#Tratamiento de Outliers columna total_amount\n",
    "Q1 = df_Impar[\"total_amount\"].quantile(0.01)\n",
    "Q3 = df_Impar[\"total_amount\"].quantile(0.99)\n",
    "#Se calcula el rango intercuartilico IQR.\n",
    "IQR = Q3 -Q1\n",
    "outliers_Sup_total_a = (Q3 + (1.5*IQR)) \n",
    "df_Impar[\"total_amount_out\"] = df_Impar.apply(lambda row: 1 if row[\"tip_amount\"]>outliers_Sup_total_a else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recordar etapa uno borrar si todos los 3 valores importantes son outliers. las propinas se tratan de otra manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etapa 2: si trip distance es outlier, pero fare amount no, podemos imputar el valor como hicimos con los que estaban en cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kms(df_Impar, ID, avg_km):\n",
    "    df_Impar[\"trip_distance\"] = df_Impar.apply(lambda row: row[\"fare_amount\"]*avg_km if row[\"trip_distance_out\"]==1  and row[\"fare_amount_out\"]==0 and row[\"RatecodeID\"]==ID else row[\"trip_distance\"], axis=1)\n",
    "    return df_Impar[\"trip_distance\"]\n",
    "\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 1, avg_km1)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 2, avg_km2)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 3, avg_km3)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 4, avg_km4)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 5, avg_km4)\n",
    "\n",
    "#los outliers restantes se borran.\n",
    "df_Impar = df_Impar.drop(df_Impar[df_Impar.trip_distance>outliers_Sup_trip].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etapa 3, si fare amount es un outlier, pero trip distance no Y NO ES CERO, podriamos imputarle un valor segun su ratecode id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kms(df_Impar, ID, avg_km):\n",
    "    df_Impar[\"fare_amount\"] = df_Impar.apply(lambda row: row[\"trip_distance\"]/avg_km if row[\"fare_amount_out\"]==1  and row[\"trip_distance\"]!=0 and row[\"RatecodeID\"]==ID else row[\"trip_distance\"], axis=1)\n",
    "    return df_Impar[\"trip_distance\"]\n",
    "\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 1, avg_km1)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 2, avg_km2)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 3, avg_km3)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 4, avg_km4)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 5, avg_km4)\n",
    "\n",
    "#se dropean los que quedaron ouliers\n",
    "df_Impar = df_Impar.drop(df_Impar[df_Impar.fare_amount>outliers_Sup_fare].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etapa 4: si fare amount es cero, pero trip distance no, tenemos que imputar los valores de igual manera que si fuese un outlier. \n",
    "\n",
    "como hay solo 6 datos en cero, los borramos por ser insignificantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar = df_Impar.drop(df_Impar[df_Impar.fare_amount==0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etapa 5: si total_amount es un outlier, deberiamos imputar el valor segun su trip distance segun su ratecode id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kms(df_Impar, ID, avg_km):\n",
    "    df_Impar[\"total_amount\"] = df_Impar.apply(lambda row: ((row[\"trip_distance\"]/avg_km)+row[\"extra\"]+row[\"mta_tax\"]+row[\"tip_amount\"]+row[\"tolls_amount\"]+row[\"improvement_surcharge\"]) if row[\"total_amount_out\"]==1  and row[\"trip_distance\"]!=0 and row[\"RatecodeID\"]==ID else row[\"trip_distance\"], axis=1)\n",
    "    return df_Impar[\"trip_distance\"]\n",
    "\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 1, avg_km1)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 2, avg_km2)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 3, avg_km3)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 4, avg_km4)\n",
    "df_Impar[\"trip_distance\"] = kms(df_Impar, 5, avg_km4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar.to_parquet('C:/Users/andre/OneDrive/Escritorio/proyecto final 3/Proyecto-Taxis-NYC/data/df_Impar_marcado.parquet',engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 verificar outliers de duracion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
