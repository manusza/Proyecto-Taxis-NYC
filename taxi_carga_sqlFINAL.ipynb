{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns=None\n",
    "from datetime import datetime as dt\n",
    "import glob\n",
    "import shutil\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "#import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_list=[]\n",
    "try:\n",
    "  for file in glob.glob('**/yellow_t*.parquet'):\n",
    "    df=pd.read_parquet(file, engine=\"pyarrow\")\n",
    "    df_list.append(df)\n",
    "  df=pd.concat(df_list)\n",
    "except:\n",
    "  print(\"No hay archivos para cargar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se agrego la columna IdFecha a la funcion t_datime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_11468\\4200162722.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['IdSemana']= df['tpep_pickup_datetime'].dt.week\n"
     ]
    }
   ],
   "source": [
    "def t_datime(df):\n",
    "    \n",
    "    df = df[df[\"tpep_pickup_datetime\"] >= '2018-01-01 00:00:00']\n",
    "    df = df[df[\"tpep_pickup_datetime\"] <= dt.now()]\n",
    "    df['IdSemana']= df['tpep_pickup_datetime'].dt.week\n",
    "    df[\"IdFecha\"] = df['tpep_pickup_datetime'].dt.strftime('%Y%m%d%H')\n",
    "    df.IdFecha = df.IdFecha.astype(\"int\")\n",
    "    df.drop(columns=[\"congestion_surcharge\",\"airport_fee\",\"store_and_fwd_flag\"], inplace=True)\n",
    "    df.sort_values(by='tpep_pickup_datetime', ascending=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def t_int(df):\n",
    "    df['passenger_count'] = pd.to_numeric(df['passenger_count'], downcast=\"integer\" )\n",
    "    df['RatecodeID'] = pd.to_numeric(df['RatecodeID'], downcast=\"integer\" )\n",
    "    df['PULocationID'] = pd.to_numeric(df['PULocationID'], downcast=\"integer\" )\n",
    "    df['DOLocationID'] = pd.to_numeric(df['DOLocationID'], downcast=\"integer\" )\n",
    "    df['payment_type'] = pd.to_numeric(df['payment_type'], downcast=\"integer\" )\n",
    "    df['RatecodeID'] = pd.to_numeric(df['RatecodeID'], downcast=\"integer\" )\n",
    "    df['RatecodeID'] = pd.to_numeric(df['RatecodeID'], downcast=\"integer\" )\n",
    "    return df\n",
    "\n",
    "def t_float(df):\n",
    "    df['trip_distance'] = pd.to_numeric(df['trip_distance'], downcast=\"float\" )\n",
    "    df['fare_amount'] = pd.to_numeric(df['fare_amount'], downcast=\"float\" )\n",
    "    df['extra'] = pd.to_numeric(df['extra'], downcast=\"float\" )\n",
    "    df['mta_tax'] = pd.to_numeric(df['mta_tax'], downcast=\"float\" )\n",
    "    df['tip_amount'] = pd.to_numeric(df['tip_amount'], downcast=\"float\" )\n",
    "    df['tolls_amount'] = pd.to_numeric(df['tolls_amount'], downcast=\"float\" )\n",
    "    df['improvement_surcharge'] = pd.to_numeric(df['improvement_surcharge'], downcast=\"float\" )\n",
    "    df['total_amount'] = pd.to_numeric(round(df['total_amount'] ))\n",
    "    return df\n",
    "\n",
    "def total_transform(df):\n",
    "    df = t_datime(df)\n",
    "    df = t_int(df)\n",
    "    df = t_float(df)\n",
    "    return df\n",
    "\n",
    "df =  total_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boroughs (df_Impar):\n",
    "    #se carga el csv con las zonas\n",
    "    zona = pd.read_csv(\"taxi+_zone_lookup.csv\")\n",
    "    #se borran las columnas que no usamos\n",
    "    zona.drop(columns=[\"Zone\",\"service_zone\"], inplace=True)\n",
    "    #cambiamos el nombre para poder hacer el merge\n",
    "    zona.rename(columns={\"LocationID\": \"PULocationID\"},inplace=True)\n",
    "    #se hace el merge y se crea la columnna resultante del merge \"Borough\"\n",
    "    df_Impar = pd.merge(df_Impar,zona, how=\"left\", on=[\"PULocationID\"])\n",
    "    #se le cambia el nombre a \"boroug\" para que haga referencia al borough de partida\n",
    "    df_Impar.rename(columns={\"Borough\": \"PUBorough\"},inplace=True)\n",
    "    #mismo procedimiento para borough de llegada\n",
    "    zona.rename(columns={\"PULocationID\": \"DOLocationID\"},inplace=True)\n",
    "    df_Impar = pd.merge(df_Impar,zona, how=\"left\", on=[\"DOLocationID\"])\n",
    "    df_Impar.rename(columns={\"Borough\": \"DOLocation\"},inplace=True)\n",
    "    return df_Impar\n",
    "\n",
    "df_Impar = boroughs(df_Impar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cambian los borough por los idBorough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"EWR\",1,df_Impar['PUBorough'])\n",
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"Queens\",2,df_Impar['PUBorough'])\n",
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"Bronx\",3,df_Impar['PUBorough'])\n",
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"Manhattan\",4,df_Impar['PUBorough'])\n",
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"Staten Island\",5,df_Impar['PUBorough'])\n",
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"Brooklyn\",6,df_Impar['PUBorough'])\n",
    "df_Impar['PUBorough']=np.where(df_Impar['PUBorough']==\"Unknown\",7,df_Impar['PUBorough'])\n",
    "\n",
    "df_Impar.rename(columns={\"DOLocation\":\"DOBorough\"},inplace=True)\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"EWR\",1,df_Impar['DOBorough'])\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"Queens\",2,df_Impar['DOBorough'])\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"Bronx\",3,df_Impar['DOBorough'])\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"Manhattan\",4,df_Impar['DOBorough'])\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"Staten Island\",5,df_Impar['DOBorough'])\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"Brooklyn\",6,df_Impar['DOBorough'])\n",
    "df_Impar['DOBorough']=np.where(df_Impar['DOBorough']==\"Unknown\",7,df_Impar['DOBorough'])\n",
    "\n",
    "\n",
    "df_Impar = df_Impar[df_Impar[\"PUBorough\"] != 7]\n",
    "df_Impar = df_Impar[df_Impar[\"DOBorough\"] != 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar.drop(columns={\"PULocationID\",\"DOLocationID\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_values(df_Impar):   \n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.fare_amount<0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.trip_distance<0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.total_amount<0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.RatecodeID==6].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.RatecodeID==99].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.trip_distance==0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.fare_amount==0].index)\n",
    "    df_Impar = df_Impar.drop(df_Impar[df_Impar.total_amount==0].index)\n",
    "    return df_Impar\n",
    "\n",
    "df_Impar=neg_values(df_Impar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_outliers(column):\n",
    "    Q1 = df_Impar[column].quantile(0.01)\n",
    "    Q3 = df_Impar[column].quantile(0.99)\n",
    "    #Se calcula el rango intercuartilico IQR.\n",
    "    IQR = Q3 -Q1\n",
    "    outliers_Sup_trip = (Q3 + (1.5*IQR)) \n",
    "    return outliers_Sup_trip\n",
    "\n",
    "trip_distance = t_outliers(\"trip_distance\")\n",
    "fare_amount = t_outliers(\"fare_amount\")\n",
    "tip_amount = t_outliers(\"tip_amount\")\n",
    "total_amount = t_outliers(\"total_amount\")\n",
    "\n",
    "df_Impar[\"outlier\"] = df_Impar.apply(lambda row: 1 if row[\"trip_distance\"]>trip_distance or row[\"fare_amount\"]>fare_amount or\n",
    "row[\"total_amount\"]>total_amount or row[\"tip_amount\"]>tip_amount else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizacion de columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar.rename(columns={\"VendorID\": \"IdVendor\",\"RatecodeID\":\"IdRatecode\",\"payment_type\":\"IdPayment_Type\",\n",
    "\"PUBorough\":\"IdPUBorough\",\"DOBorough\":\"IdDOBorough\" },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Impar['IdPUBorough'] = pd.to_numeric(df_Impar['IdPUBorough'], downcast=\"integer\" )\n",
    "df_Impar['IdDOBorough'] = pd.to_numeric(df_Impar['IdDOBorough'], downcast=\"integer\" )\n",
    "df_Impar['outlier'] = pd.to_numeric(df_Impar['outlier'], downcast=\"integer\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se terminan los cambios en df_Impar y queda listo para ingestar\n",
    "hacer indice auto incremental en sql."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabaja con el clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[]\n",
    "    \n",
    "for file in glob.glob('**/new york 20*.csv'):\n",
    "  df=pd.read_csv(file,delimiter = ',',encoding = \"utf-8\")\n",
    "  df_list.append(df)\n",
    "clima=pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clima = pd.read_csv(\"C:/Users/andre/OneDrive/Escritorio/proyecto final 3/Proyecto-Taxis-NYC/clima.csv\")\n",
    "clima.drop(columns=[\"name\",\"dew\",\"humidity\",\"precipprob\",\"preciptype\",\"severerisk\",\"uvindex\",\"icon\",\"stations\"], inplace=True)\n",
    "clima.drop(columns=[\"windgust\",\"windspeed\",\"winddir\",\"sealevelpressure\",\"cloudcover\",\"visibility\",\"solarradiation\",\"solarenergy\"], inplace=True)\n",
    "clima[\"datetime\"]=[x.replace('T',\" \") for x in clima[\"datetime\"]]\n",
    "clima.datetime = clima['datetime'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_clima():\n",
    "    clima['temp'] = pd.to_numeric(clima['temp'], downcast=\"float\" )\n",
    "    clima['feelslike'] = pd.to_numeric(round(clima['feelslike'] ))\n",
    "    clima['precip'] = pd.to_numeric(clima['precip'], downcast=\"float\" )\n",
    "    clima['snow'] = pd.to_numeric(clima['snow'], downcast=\"float\" )\n",
    "    clima['snowdepth'] = pd.to_numeric(clima['snowdepth'], downcast=\"float\" )\n",
    "    clima[\"conditions\"]=clima[\"conditions\"].astype(str)\n",
    "    clima.rename(columns={\"datetime\": \"hora\"},inplace=True)\n",
    "    #Creamos IdFecha\n",
    "    clima[\"IdFecha\"] = clima['hora'].dt.strftime('%Y%m%d%H')\n",
    "    clima.IdFecha = clima.IdFecha.astype(\"int\")\n",
    "    #Cambio de variable\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Clear\",1,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Partially cloudy\",2,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Overcast\",3,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Snow, Overcast\",4,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Snow, Partially cloudy\",5,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Snow\",6,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Rain, Overcast\",7,clima['conditions'])\n",
    "    clima['conditions']=np.where(clima['conditions']==\"Rain, Partially cloudy\",8,clima['conditions'])\n",
    "    clima['conditions'] = pd.to_numeric(clima['conditions'], downcast=\"integer\" )\n",
    "    return clima\n",
    "\n",
    "clima = t_clima()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aca trabajo con clima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrancamos con sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as msql\n",
    "from mysql.connector import Error\n",
    "from sqlalchemy import create_engine\n",
    "from multiprocessing import connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_Impar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credenciales:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name,db_name, u_name, u_pass, port_num  = \"127.0.0.1\",\"taxi_2\", \"root\", \"Andres1\", \"3306\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion del motor de base de datos\n",
    "engine = create_engine(\"mysql+mysqlconnector://\" + u_name + \":\" + u_pass + \"@\" \n",
    "                        + host_name + \":\" + port_num + \"/\" + db_name, echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tablas para sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tabla clima conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clima_c():\n",
    "    Clima_conditions = clima.IdCondiciones.unique()\n",
    "    Clima_conditions = pd.DataFrame(Clima_conditions)\n",
    "    Clima_conditions[\"conditions\"] = Clima_conditions\n",
    "    Clima_conditions = Clima_conditions.drop(columns=0)\n",
    "    Clima_conditions.reset_index(inplace=True)\n",
    "    Clima_conditions.rename(columns={\"index\":\"IdCondiciones\"},inplace=True)\n",
    "    Clima_conditions.conditions.replace(1,\"Clear\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(2,\"Partially cloudy\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(3,\"Overcast\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(4,\"Snow, Overcast\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(5,\"Snow, Partially cloudy\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(6,\"Snow\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(7,\"Rain, Overcast\", inplace=True)\n",
    "    Clima_conditions.conditions.replace(8,\"Rain, Partially cloudy\", inplace=True)\n",
    "    Clima_conditions[\"IdCondiciones\"] = Clima_conditions.apply(lambda row: row[\"IdCondiciones\"]+1 , axis=1)\n",
    "\n",
    "    Clima_conditions.to_sql(name=\"clima_conditions\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tabla Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boro():\n",
    "    Borough = pd.read_csv(\"Taxi+_zone_lookup.csv\", usecols=['Borough'])\n",
    "    Borough = Borough.Borough.unique()\n",
    "    Borough = pd.DataFrame(Borough)\n",
    "    Borough[\"Borough\"] = Borough\n",
    "    Borough = Borough.drop(columns=0)\n",
    "    Borough.reset_index(inplace=True)\n",
    "    Borough.rename(columns={\"index\":\"IdBorough\"},inplace=True)\n",
    "    Borough[\"IdBorough\"] = Borough.apply(lambda row: row[\"IdBorough\"]+1 , axis=1)\n",
    "\n",
    "    Borough.to_sql(name=\"borough\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tabla vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vendor():\n",
    "    Vendor = df_Impar.IdVendor.unique()\n",
    "    Vendor = pd.DataFrame(Vendor)\n",
    "    Vendor[\"IdVendor\"] = Vendor\n",
    "    Vendor = Vendor.drop(columns=0)\n",
    "    Vendor[\"Vendor\"] = Vendor.IdVendor\n",
    "    Vendor.Vendor.replace(1,\"Tecnologías móviles creativas\", inplace=True)\n",
    "    Vendor.Vendor.replace(2,\"VeriFone Inc.\", inplace=True)\n",
    "\n",
    "    Vendor.to_sql(name=\"vendor\", con=engine, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tabla paymentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payment():\n",
    "    PaymentType = df_Impar.IdPayment_Type.unique()\n",
    "    PaymentType = pd.DataFrame(PaymentType)\n",
    "    PaymentType[\"IdPayment_Type\"] = PaymentType\n",
    "    PaymentType = PaymentType.drop(columns=0)\n",
    "    PaymentType[\"Payment\"] = PaymentType.IdPayment_Type\n",
    "    PaymentType.Payment.replace(1,\"tarjeta de crédito\", inplace=True)\n",
    "    PaymentType.Payment.replace(2,\"Efectivo\", inplace=True)\n",
    "    PaymentType.Payment.replace(3,\"Sin cargo\", inplace=True)\n",
    "    PaymentType.Payment.replace(4,\"Disputa\", inplace=True)\n",
    "\n",
    "    PaymentType.to_sql(name=\"payment_type\", con=engine, chunksize=200, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tabla rate code ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_code():\n",
    "    RatecodeID = df_Impar.IdRatecode.unique()\n",
    "    RatecodeID = pd.DataFrame(RatecodeID)\n",
    "    RatecodeID[\"IdRatecode\"] = RatecodeID\n",
    "    RatecodeID = RatecodeID.drop(columns=0)\n",
    "    RatecodeID[\"Ratecode\"] = RatecodeID.IdRatecode\n",
    "    RatecodeID.Ratecode.replace(1,\"Tarifa estándar\", inplace=True)\n",
    "    RatecodeID.Ratecode.replace(2,\"JFK\", inplace=True)\n",
    "    RatecodeID.Ratecode.replace(3,\"Nueva York\", inplace=True)\n",
    "    RatecodeID.Ratecode.replace(4,\"Nasáu o Westchester\", inplace=True)\n",
    "    RatecodeID.Ratecode.replace(5,\"Tarifa negociada\", inplace=True)\n",
    "\n",
    "    RatecodeID.to_sql(name=\"ratecode\", con=engine, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clima= clima.rename(columns={'conditions':'IdCondiciones'}) #AGREGUE ESTA\n",
    "clima.to_sql(name=\"clima_aux\", con=engine, if_exists=\"append\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ACOMODAR df_zone BOROUGH CON ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone():\n",
    "    df_zone=pd.read_csv(\"taxi+_zone_lookup.csv\",sep=\",\")\n",
    "\n",
    "    df_zone.rename(columns={\"LocationID\":\"IdZona\"},inplace=True)\n",
    "    df_zone.rename(columns={\"Borough\":\"IdBorough\"},inplace=True)\n",
    "    df_zone.IdBorough.replace(\"Queens\",2, inplace=True)\n",
    "    df_zone.IdBorough.replace(\"Bronx\",3, inplace=True)\n",
    "    df_zone.IdBorough.replace(\"Manhattan\",4, inplace=True)\n",
    "    df_zone.IdBorough.replace(\"Staten Island\",5, inplace=True)\n",
    "    df_zone.IdBorough.replace(\"Brooklyn\",6, inplace=True)\n",
    "    df_zone.IdBorough.replace(\"Unknown\",7, inplace=True)\n",
    "    df_zone.IdBorough.replace(\"EWR\",1, inplace=True)\n",
    "\n",
    "    df_zone.to_sql(name=\"zona\", con=engine, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_init():\n",
    "    if os.path.isfile('delta.csv'):\n",
    "        pass\n",
    "    else:\n",
    "        clima_c()\n",
    "        boro()\n",
    "        vendor()\n",
    "        payment()\n",
    "        rate_code()\n",
    "        zone()     \n",
    "\n",
    "switch_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"delta.csv\",index=False, sep= ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion(file):\n",
    "    connection = msql.Connect(host=host_name, user=u_name, passwd=u_pass, db=db_name)\n",
    "    cursor = connection.cursor()\n",
    "    query = \"LOAD DATA INFILE 'C:/Users/andre/OneDrive/Escritorio/proyecto final 3/Proyecto-Taxis-NYC/%s' INTO TABLE taxis_aux FIELDS TERMINATED BY ';' ENCLOSED BY '' ESCAPED BY '' IGNORE 1 LINES\" % file\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "\n",
    "conexion('delta.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = msql.Connect(host=host_name, user=u_name, passwd=u_pass, db=db_name)\n",
    "cursor = connection.cursor()\n",
    "query = \"CALL traerUnicos()\" \n",
    "cursor.execute(query)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = msql.Connect(host=host_name, user=u_name, passwd=u_pass, db=db_name)\n",
    "cursor = connection.cursor()\n",
    "query = \"CALL traerUnicosClima()\" \n",
    "cursor.execute(query)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/cargados\\\\yellow_tripdata_2018-02.parquet'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=glob.glob('**/yellow_t*.parquet')\n",
    "for i in range(0,len(a)):\n",
    "  shutil.move(a[i], \"data/cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/cargados\\\\new york 2018-01-31 to 2018-02-28.csv'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=glob.glob('**/new york 20*.csv')\n",
    "for i in range(0,len(a)):\n",
    "  shutil.move(a[i], \"data/cargados\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
